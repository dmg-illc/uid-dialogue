{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b699dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import json \n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data_dir = '/Users/mario/code/exp-rep/data/BNC-2014/two-speakers/analysis/'\n",
    "dialign_output = analysis_data_dir + 'dialign-output/'\n",
    "\n",
    "# dialign_output += 'nopos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73521285",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_lexica = {}\n",
    "for f in os.listdir(dialign_output + 'nopos/'):\n",
    "    if f.endswith('_tsv-lexicon.tsv') and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'nopos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in shared_lexica:\n",
    "            shared_lexica[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            shared_lexica[dial_id] = pd.concat([shared_lexica[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "self_lexica = {}\n",
    "for f in os.listdir(dialign_output + 'nopos/'):\n",
    "    if (f.endswith('_tsv-lexicon-self-rep-A.tsv') or f.endswith('_tsv-lexicon-self-rep-B.tsv')) and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'nopos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in self_lexica:\n",
    "            self_lexica[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            self_lexica[dial_id] = pd.concat([self_lexica[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "print(len(shared_lexica), len(self_lexica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca41764",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_lexica_pos = {}\n",
    "for f in os.listdir(dialign_output + 'pos/'):\n",
    "    if f.endswith('_tsv-lexicon.tsv') and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'pos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in shared_lexica_pos:\n",
    "            shared_lexica_pos[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            shared_lexica_pos[dial_id] = pd.concat([shared_lexica_pos[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "\n",
    "self_lexica_pos = {}\n",
    "for f in os.listdir(dialign_output + 'pos/'):\n",
    "    if (f.endswith('_tsv-lexicon-self-rep-A.tsv') or f.endswith('_tsv-lexicon-self-rep-B.tsv')) and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'pos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in self_lexica_pos:\n",
    "            self_lexica_pos[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            self_lexica_pos[dial_id] = pd.concat([self_lexica_pos[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "print(len(shared_lexica_pos), len(self_lexica_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f169eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(analysis_data_dir + 'contexts.json', 'r') as f:\n",
    "    contexts = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_lexica_pos['SJV7'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topical_or_referential(word_seq, pos_seq):\n",
    "    assert len(word_seq) == len(pos_seq), (word_seq, pos_seq)\n",
    "    \n",
    "    GENERIC_NOUNS = 'bit bunch fact god middle ones part rest side sort sorts stuff thanks loads lot lots kind kinds time times way ways problem problems thing things idea ideas reason reasons day days week weeks year years'\n",
    "    GENERIC_NOUNS = GENERIC_NOUNS.split(' ')\n",
    "    \n",
    "    if pos_seq.count('SUBST') >= 1:\n",
    "        is_generic = True\n",
    "        for w, tag in zip(word_seq, pos_seq):\n",
    "            if tag == 'SUBST' and w not in GENERIC_NOUNS:\n",
    "                is_generic = False\n",
    "                break\n",
    "        if not is_generic:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def more_than_half_filled_pauses(construction):\n",
    "    construction = construction.split(' ')\n",
    "    FILLED_PAUSES = ['huh', 'uh', 'erm', 'hm', 'mm', 'er']\n",
    "    n_filled_pauses = 0.\n",
    "    for w in construction:\n",
    "        if w in FILLED_PAUSES:\n",
    "            n_filled_pauses += 1\n",
    "    return n_filled_pauses >= len(construction) / 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_half_filled_pauses('mm mm mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b201658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_constructions = {}\n",
    "pos_tagged_constructions_topical = {}\n",
    "\n",
    "for d_id in shared_lexica_pos:\n",
    "    lexicon_df = pd.concat((self_lexica_pos[d_id], shared_lexica_pos[d_id]))\n",
    "    for _, row in lexicon_df.iterrows():\n",
    "        constr = row['Surface Form']\n",
    "        \n",
    "        if not isinstance(constr, str):\n",
    "            continue\n",
    "        constr = constr.replace('? #STOP', '?#STOP')\n",
    "        constr = constr.strip()\n",
    "            \n",
    "        tokens = constr.split(' ')\n",
    "        w_seq = []\n",
    "        pos_seq = []\n",
    "        illegal_constr = False\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                w, tag = token.split('#')\n",
    "            except ValueError:\n",
    "                illegal_constr = True \n",
    "            w_seq.append(w)\n",
    "            pos_seq.append(tag)\n",
    "        \n",
    "        if illegal_constr:\n",
    "            print('Illegal construction:', constr)\n",
    "            continue   # only exception is: \"made . com#SUBST\"\n",
    "            \n",
    "        concat_tokens = ''.join(w_seq)\n",
    "        \n",
    "        # Referential or topical constructions?\n",
    "        if topical_or_referential(w_seq, pos_seq):\n",
    "            pos_tagged_constructions_topical[concat_tokens] = pos_seq\n",
    "        else:\n",
    "            pos_tagged_constructions[concat_tokens] = pos_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts['SJV7']['205']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subsequence(subsequence, sequence):\n",
    "    l = len(subsequence)\n",
    "    ranges = []\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i+l] == subsequence:\n",
    "            if i - 1 < 0:\n",
    "                space_before = True\n",
    "            else:\n",
    "                space_before = sequence[i-1] in \" ',.!:;?\"\n",
    "  \n",
    "            if i + l >= len(sequence):\n",
    "                space_after = True\n",
    "            else:\n",
    "                space_after = sequence[i+l] in \" ',.!:;?\"\n",
    "                \n",
    "            if space_before and space_after:\n",
    "                ranges.append((i, i+l))\n",
    "    return ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = \"and i was just like\"\n",
    "s = \"and i was just like oh my god\"\n",
    "find_subsequence(ss, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c27653",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = \"bad for you\"\n",
    "s = \"yeah it is bad for you bad for your teeth\"\n",
    "find_subsequence(ss, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409720e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_counts = defaultdict(lambda: {}) #[dialogue][expression]\n",
    "_data = defaultdict(lambda: {})\n",
    "\n",
    "cnt = []\n",
    "for d_id in tqdm(shared_lexica):\n",
    "    lexicon_df = pd.concat((self_lexica[d_id], shared_lexica[d_id]))\n",
    "    dialogue = contexts[d_id]\n",
    "    \n",
    "    for _, row in lexicon_df.iterrows():\n",
    "        constr = row['Surface Form']\n",
    "        \n",
    "        if not isinstance(constr, str):\n",
    "            continue\n",
    "        constr = constr.strip()\n",
    "            \n",
    "        turns = row['Turns'].split(', ')\n",
    "        \n",
    "        _freq = 0\n",
    "        for turn in turns:\n",
    "            _, _, text = dialogue[turn]\n",
    "            ranges = find_subsequence(constr, text)\n",
    "            _freq += len(ranges)\n",
    "        \n",
    "        assert _freq >= row['Freq.']\n",
    "        \n",
    "        # Condition 1: at least 3 tokens long\n",
    "        if row['Size'] < 3:\n",
    "            continue\n",
    "        \n",
    "        # Condition 2: frequency >= 3 in the dialogue\n",
    "        if _freq < 3:\n",
    "            continue\n",
    "            \n",
    "        # Condition 3: free form frequency >= 2 in the dialogue\n",
    "        if row['Free Freq.'] < 2:\n",
    "            continue\n",
    "        \n",
    "        concat_tokens = constr.replace(' ', '')\n",
    "        if concat_tokens in pos_tagged_constructions_topical:\n",
    "            _pos_seq = pos_tagged_constructions_topical[concat_tokens]\n",
    "            topical = True\n",
    "            cnt.append(1)\n",
    "        elif concat_tokens in pos_tagged_constructions:\n",
    "            _pos_seq = pos_tagged_constructions[concat_tokens]\n",
    "            topical = False\n",
    "            cnt.append(1)\n",
    "        else:\n",
    "            # Skip constructions for which we find no POS-tagged equivalent\n",
    "            cnt.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Condition 4: no punctuation in the construction\n",
    "        if \"STOP\" in _pos_seq:\n",
    "            continue\n",
    "            \n",
    "        # Condition 5: at least half of the construction should not correspond to filled pauses\n",
    "        if more_than_half_filled_pauses(constr):\n",
    "            continue\n",
    "        \n",
    "        _data[d_id][constr] = {\n",
    "            'Frequency': _freq,\n",
    "            'Free frequency': row['Free Freq.'],\n",
    "            'Length': row['Size'],\n",
    "            'POS sequence': _pos_seq,\n",
    "            'First speaker': row['First Speaker'],\n",
    "            'Turns': turns, \n",
    "            'Spanning turns': row['Spanning'],\n",
    "            'Establishment turn': row['Establishment turn'],\n",
    "            'Topical': topical\n",
    "        }\n",
    "            \n",
    "        corpus_counts[d_id][constr] = _freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skipped {} out of {} constructions ({:.2f}%) as we find no POS-tagged equivalent.'.format(\n",
    "    len([x for x in cnt if x == 0]),\n",
    "    len(cnt),\n",
    "    len([x for x in cnt if x == 0]) / len(cnt) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ff60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueSpecificity:\n",
    "    \n",
    "    def __init__(self, corpus_counts):\n",
    "        \"\"\"\n",
    "        Dialogue specificity statistics for expressions in a dialogue corpus.\n",
    "        \n",
    "        Args\n",
    "            corpus_counts: A dictionary of dictionaries:\n",
    "                           corpus_counts[dialogue][expression] = expression frequency in dialogue \n",
    "        \"\"\"\n",
    "        self.corpus_counts = corpus_counts\n",
    "        self.dialogues = list(self.corpus_counts.keys())\n",
    "        \n",
    "        # Probability distribution over dialogues is uniform -- P(dial)\n",
    "        self.p_dial = 1 / len(corpus_counts.keys()) \n",
    "        \n",
    "        \n",
    "        # The number of expressions (tokens) in a dialogue -- N_dial\n",
    "        self.n_tokens_in_dial = {}  # N_dial\n",
    "        for dial in corpus_counts:\n",
    "            self.n_tokens_in_dial[dial] = sum(corpus_counts[dial].values())\n",
    "        \n",
    "        \n",
    "        # The (token) frequency of an expression in the corpus -- N_exp\n",
    "        self.exp_freq_in_corpus = defaultdict(int)\n",
    "        \n",
    "        # The total number of expressions (tokens) in the corpus -- N_corpus\n",
    "        self.n_exp_tokens = 0  # N_corpus\n",
    "        \n",
    "        # The probability of an expression in the corpus -- P(exp)\n",
    "        self.p_exp = defaultdict(int)\n",
    "        \n",
    "        # The probability of an expression in a dialogue -- P(exp|dial)\n",
    "        self.p_exp_given_dial = defaultdict(int)\n",
    "        \n",
    "        \n",
    "        for dial in corpus_counts:\n",
    "            for exp in corpus_counts[dial]:\n",
    "                self.exp_freq_in_corpus[exp] += corpus_counts[dial][exp]\n",
    "                self.n_exp_tokens += corpus_counts[dial][exp]\n",
    "                \n",
    "                # P(exp|dial) = fr(exp, dial) / N_exp\n",
    "                self.p_exp_given_dial[(exp, dial)] = self.corpus_counts[dial][exp] / self.n_tokens_in_dial[dial]\n",
    "                \n",
    "                # P(exp) = sum(dial' in corpus) [ P(exp|dial') * P(dial') ]\n",
    "                self.p_exp[exp] += self.p_exp_given_dial[(exp, dial)] * self.p_dial\n",
    "        \n",
    "        \n",
    "        # The total number of expression types in the corpus -- E_corpus\n",
    "        self.n_exp_types = len(self.exp_freq_in_corpus)\n",
    "                \n",
    "                \n",
    "        # P(dial|exp) for all expressions in all dialogues\n",
    "        self.dialogue_posteriors = {}\n",
    "        for dial in self.corpus_counts:\n",
    "            self.dialogue_posteriors[dial] = self.get_dialogue_posteriors(dial)\n",
    "            \n",
    "        # Specificity [P(exp|dial) - P(exp)] for all expressions in all dialogues\n",
    "        self.dialogue_specificity = {}\n",
    "        for dial in self.corpus_counts:\n",
    "            self.dialogue_specificity[dial] = self.get_dialogue_specificity(dial)  \n",
    "            \n",
    "        # PMI(exp, dial) for all expressions in all dialogues\n",
    "        self.pmi = {}\n",
    "        for dial in self.corpus_counts:\n",
    "            self.pmi[dial] = self.get_dialogue_pmi(dial)\n",
    "            \n",
    "        # MD(exp, dial) for all expressions in all dialogues\n",
    "        self.mutual_dependency = {}\n",
    "        for dial in self.corpus_counts:\n",
    "            self.mutual_dependency[dial] = self.get_mutual_dependency(dial)\n",
    "            \n",
    "        # LFMD(exp, dial) for all expressions in all dialogues\n",
    "        self.lf_mutual_dependency = {}\n",
    "        for dial in self.corpus_counts:\n",
    "            self.lf_mutual_dependency[dial] = self.get_lfmd(dial)\n",
    "            \n",
    "        \n",
    "    def posterior(self, expression, dialogue):\n",
    "        \"\"\"\n",
    "        Compute P(dialogue|expression) for a given expression in a given dialogue.\n",
    "        P(dialogue|expression) = P(expression|dialogue) * P(dialogue) / P(expression)\n",
    "        \"\"\" \n",
    "        return self.p_exp_given_dial[(expression, dialogue)] * self.p_dial / self.p_exp[expression]\n",
    "    \n",
    "    \n",
    "    def get_dialogue_posteriors(self, dialogue):\n",
    "        \"\"\"\n",
    "        Compute P(dialogue|expression) for all expressions in a dialogue.\n",
    "        \"\"\"\n",
    "        posteriors = Counter()\n",
    "        for exp in self.corpus_counts[dialogue]:\n",
    "            posteriors[exp] = self.posterior(exp, dialogue)\n",
    "        return posteriors\n",
    "    \n",
    "    \n",
    "    def get_dialogue_specificity(self, dialogue):\n",
    "        \"\"\"\n",
    "        Compute P(expression|dialogue) - P(expression) for all expressions in a dialogue.\n",
    "        \"\"\"\n",
    "        specificity = Counter()\n",
    "        for exp in self.corpus_counts[dialogue]:\n",
    "            specificity[exp] = self.p_exp_given_dial[(exp, dialogue)] - self.p_exp[exp]\n",
    "        return specificity\n",
    "    \n",
    "    \n",
    "    def get_dialogue_pmi(self, dialogue):\n",
    "        \"\"\"\n",
    "        Compute pointwise mutual information for all expressions in a dialogue.\n",
    "        PMI(expression, dialogue) = log[ P(expression|dialogue) / P(expression) ]\n",
    "        \"\"\"\n",
    "        pmi = Counter()\n",
    "        for exp in self.corpus_counts[dialogue]:\n",
    "            pmi[exp] = np.log2(self.p_exp_given_dial[(exp, dialogue)] / self.p_exp[exp])\n",
    "        return pmi\n",
    "    \n",
    "    \n",
    "    def get_mutual_dependency(self, dialogue):\n",
    "        \"\"\"\n",
    "        Compute mutual dependence MD(expression, dialogue) for all expressions in a dialogue (Thanopoulos et al, 2002).\n",
    "        MD(expression, dialogue) = log[ P(expression|dialogue)**2 * P(dial) / P(expression) ]\n",
    "        \"\"\"\n",
    "        md = Counter()\n",
    "        for exp in self.corpus_counts[dialogue]:\n",
    "            md[exp] = np.log2(((self.p_exp_given_dial[(exp, dialogue)] ** 2) * self.p_dial) / self.p_exp[exp])\n",
    "        return md\n",
    "    \n",
    "\n",
    "    def get_lfmd(self, dialogue):\n",
    "        \"\"\"\n",
    "        Compute log-frequency biased mutual dependence LFMD(expression, dialogue) for all expressions in a dialogue (Thanopoulos et al, 2002).\n",
    "        LFMD(expression, dialogue) = MD(expression, dialogue + log P(expression, dialogue) \n",
    "        \"\"\"\n",
    "        lfmd = Counter()\n",
    "        for exp in self.corpus_counts[dialogue]:\n",
    "            lfmd[exp] = self.mutual_dependency[dialogue][exp] + np.log2(self.p_exp_given_dial[(exp, dialogue)] * self.p_dial)\n",
    "        return lfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DialogueSpecificity(corpus_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df47844",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# For each dialogue\n",
    "for dial_id in tqdm(_data):\n",
    "    \n",
    "    for constr in _data[dial_id]:\n",
    "        \n",
    "        # if ds.pmi[dial_id][constr] <= 1:\n",
    "        #     continue\n",
    "        \n",
    "        turns = _data[dial_id][constr]['Turns']\n",
    "        \n",
    "        current_freq = 0\n",
    "        prev_turn_any = None\n",
    "        prev_turn_by_speaker = {}\n",
    "        prev_start_idx_any = None\n",
    "        prev_start_idx_by_speaker = {}\n",
    "        first_occ_start_idx = None\n",
    "        first_occ_turn = None\n",
    "        speakers = set()\n",
    "        prev_speaker = None\n",
    "        \n",
    "        for turn in turns:\n",
    "        \n",
    "            _, turn_speaker, text = contexts[dial_id][turn]\n",
    "            other_speaker = [s for s in speakers if s != turn_speaker]\n",
    "            \n",
    "            \n",
    "            if len(other_speaker) == 0:\n",
    "                other_speaker = None\n",
    "            elif len(other_speaker) == 1:\n",
    "                other_speaker = other_speaker[0]\n",
    "            else:\n",
    "                raise ValueError('There should be maximum two speakers: {}'.format(speakers))\n",
    "                        \n",
    "            ranges = find_subsequence(constr, text)\n",
    "            \n",
    "            for j, (start_idx, end_idx) in enumerate(ranges):\n",
    "                current_freq += 1\n",
    "                shared_currently = len(speakers) == 2\n",
    "                \n",
    "                if not prev_turn_any:\n",
    "                    recency_any = -1\n",
    "                    recency_same = -1\n",
    "                    recency_other = -1\n",
    "                    current_spanning = -1\n",
    "                    first_occ_start_idx = start_idx\n",
    "                    first_occ_turn = turn\n",
    "                else:\n",
    "                    \n",
    "                    if first_occ_turn == turn:\n",
    "                        current_spanning = len(text[first_occ_start_idx:start_idx].split())\n",
    "                    else:\n",
    "                        current_spanning = len(contexts[dial_id][first_occ_turn][2][first_occ_start_idx:].split())\n",
    "                        for t in range(int(first_occ_turn) + 1, int(turn)):\n",
    "                            current_spanning += len(contexts[dial_id][str(t)][2].split())\n",
    "                        current_spanning += len(text[:start_idx].split())\n",
    "                        \n",
    "                    if prev_turn_any == turn:\n",
    "                        recency_any = len(text[prev_start_idx_any:start_idx].split())\n",
    "                    else:\n",
    "                        recency_any = len(contexts[dial_id][prev_turn_any][2][prev_start_idx_any:].split())\n",
    "                        for t in range(int(prev_turn_any) + 1, int(turn)):\n",
    "                            recency_any += len(contexts[dial_id][str(t)][2].split())\n",
    "                        recency_any += len(text[:start_idx].split())               \n",
    "                    \n",
    "                    if turn_speaker not in prev_turn_by_speaker:\n",
    "                        recency_same = -1\n",
    "                    else:\n",
    "                        if prev_turn_by_speaker[turn_speaker] == turn:\n",
    "                            recency_same = len(text[prev_start_idx_by_speaker[turn_speaker]:start_idx].split())\n",
    "                        else:\n",
    "                            recency_same = len(contexts[dial_id][prev_turn_by_speaker[turn_speaker]][2][prev_start_idx_by_speaker[turn_speaker]:].split())\n",
    "                            for t in range(int(prev_turn_by_speaker[turn_speaker]) + 1, int(turn)):\n",
    "                                recency_same += len(contexts[dial_id][str(t)][2].split())\n",
    "                            recency_same += len(text[:start_idx].split())\n",
    "                    \n",
    "                    if not other_speaker in prev_turn_by_speaker:\n",
    "                        recency_other = -1\n",
    "                    else:\n",
    "                        if prev_turn_by_speaker[other_speaker] == turn:\n",
    "                            recency_other = len(text[prev_start_idx_by_speaker[other_speaker]:start_idx].split())\n",
    "                        else:\n",
    "                            recency_other = len(contexts[dial_id][prev_turn_by_speaker[other_speaker]][2][prev_start_idx_by_speaker[other_speaker]:].split())\n",
    "                            for t in range(int(prev_turn_by_speaker[other_speaker]) + 1, int(turn)):\n",
    "                                recency_other += len(contexts[dial_id][str(t)][2].split())\n",
    "                            recency_other += len(text[:start_idx].split())\n",
    "                    \n",
    "\n",
    "                prev_turn_any = turn\n",
    "                prev_start_idx_any = start_idx\n",
    "                prev_turn_by_speaker[turn_speaker] = turn\n",
    "                prev_start_idx_by_speaker[turn_speaker] = start_idx\n",
    "                \n",
    "                data[dial_id][constr].append({\n",
    "                    'CurrentTurn': turn,\n",
    "                    'PositionInTurn': (start_idx, end_idx),\n",
    "                    'IndexInTurn': j,\n",
    "                    'PreviousInSameTurn': j > 0,\n",
    "                    'FrequencyInTurn': len(ranges),\n",
    "                    'Turns': turns, \n",
    "                    'RepetitionIndex': current_freq,\n",
    "                    'Frequency': _data[dial_id][constr]['Frequency'],\n",
    "                    'FreeFrequency': _data[dial_id][constr]['Free frequency'],\n",
    "                    'Length': _data[dial_id][constr]['Length'],\n",
    "                    'POSsequence': _data[dial_id][constr]['POS sequence'],\n",
    "                    'Speaker': turn_speaker,\n",
    "                    'FirstSpeaker': _data[dial_id][constr]['First speaker'],\n",
    "                    'TotalSpanningTurns': _data[dial_id][constr]['Spanning turns'],\n",
    "                    'CurrentSpanningTokens': current_spanning,\n",
    "                    'RecencyBoth': recency_any,\n",
    "                    'RecencySame': recency_same,\n",
    "                    'RecencyOther': recency_other,\n",
    "                    'SharedCurrently': shared_currently,\n",
    "                    'Shared': _data[dial_id][constr]['Establishment turn'] >= 0,\n",
    "                    'EstablishmentTurn': _data[dial_id][constr]['Establishment turn'],\n",
    "                    'SameAsFirstSpeaker': turn_speaker == _data[dial_id][constr]['First speaker'],\n",
    "                    'SameAsPreviousSpeaker': turn_speaker == prev_speaker,\n",
    "                    'Topical': _data[dial_id][constr]['Topical'],\n",
    "                    'PMI': ds.pmi[dial_id][constr],\n",
    "                })\n",
    "                prev_speaker = turn_speaker\n",
    "                \n",
    "            speakers.add(turn_speaker)\n",
    "            assert(len(speakers) in [1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0120f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbfebd",
   "metadata": {},
   "source": [
    "## Example constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d_id in ds.pmi.keys():\n",
    "for d_id in list(np.random.choice(list(ds.pmi.keys()), 10)):\n",
    "# for d_id in ['SPXV', 'S38F', 'SAXQ', 'SJDK', 'SJV7']:\n",
    "    print(d_id)\n",
    "    print('----')\n",
    "    ii=0\n",
    "    for rank, (exp, score) in enumerate(ds.pmi[d_id].most_common()):\n",
    "        if ii >= 10:\n",
    "            break\n",
    "        ds.corpus_counts[d_id][exp]\n",
    "        if score > 1:\n",
    "            if data[d_id][exp][0]['Topical']:\n",
    "                print('\\\\textit{', exp, '}', '  &  ', sep='')\n",
    "            else:\n",
    "                print(exp, ' &  ')\n",
    "            ii += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c01c93",
   "metadata": {},
   "source": [
    "## Construction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78933a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "constr_lens = {}\n",
    "for dial in data:\n",
    "    for constr in data[dial]:\n",
    "        if constr not in constr_lens:\n",
    "            len_ = data[dial][constr][0]['Length']\n",
    "            constr_lens[constr] = len_\n",
    "\n",
    "lens = list(constr_lens.values())\n",
    "np.min(lens), np.max(lens), np.mean(lens), np.std(lens), np.median(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37277482",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(lens)\n",
    "p.axes.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5902a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total unique :', len(lens))\n",
    "for l in range(np.min(lens), np.max(lens) + 1):\n",
    "    print('{:2d}   {:4d}   {:5.2f}%'.format(l, \n",
    "                                           len([x for x in lens if x == l]), \n",
    "                                           len([x for x in lens if x == l]) / len(lens) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077e847",
   "metadata": {},
   "source": [
    "#### Remove outliers with length 9 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_lengths = [9, 12]\n",
    "\n",
    "new_data = {}\n",
    "for dial in data:\n",
    "    new_data[dial] = {}\n",
    "    for constr in data[dial]:\n",
    "        len_ = data[dial][constr][0]['Length']\n",
    "        if len_ in outlier_lengths:\n",
    "            print('Removed \"{}\", with freq. {} in dialogue {}'.format(constr, len(data[dial][constr]), dial))\n",
    "        else:\n",
    "            new_data[dial][constr] = data[dial][constr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cd583",
   "metadata": {},
   "outputs": [],
   "source": [
    "constr_lens = {}\n",
    "for dial in new_data:\n",
    "    for constr in new_data[dial]:\n",
    "        if constr not in constr_lens:\n",
    "            len_ = new_data[dial][constr][0]['Length']\n",
    "            constr_lens[constr] = len_\n",
    "\n",
    "lens = list(constr_lens.values())\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), np.median(lens))\n",
    "\n",
    "\n",
    "print('Total:', len(lens))\n",
    "for l in range(np.min(lens), np.max(lens) + 1):\n",
    "    print('{:2d}   {:4d}   {:5.2f}%'.format(l, \n",
    "                                           len([x for x in lens if x == l]), \n",
    "                                           len([x for x in lens if x == l]) / len(lens) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a64415",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('chains_all.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15c986",
   "metadata": {},
   "source": [
    "### Length across all of occurrences (not types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b63e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for dial in new_data:\n",
    "    for constr in new_data[dial]:\n",
    "        for occ in new_data[dial][constr]:\n",
    "            token_lens.append(occ['Length'])\n",
    "\n",
    "# print(np.min(token_lens), np.max(token_lens), np.mean(token_lens), np.std(token_lens), np.median(token_lens))\n",
    "\n",
    "\n",
    "print('Total:', len(token_lens))\n",
    "\n",
    "for l in range(np.min(token_lens), np.max(token_lens) + 1):\n",
    "    print('{:2d}   {:5d}   {:5.2f}%'.format(l, \n",
    "                                           len([x for x in token_lens if x == l]), \n",
    "                                           len([x for x in token_lens if x == l]) / len(token_lens) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe5dee",
   "metadata": {},
   "source": [
    "## Length of turns containing constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words_in_target_turn = []\n",
    "\n",
    "for dial_id in data:\n",
    "    for exp in data[dial_id]:\n",
    "        prev_turn = None\n",
    "        for instance in data[dial_id][exp]:\n",
    "            turn = contexts[dial_id][instance['CurrentTurn']]\n",
    "            text = turn[2]\n",
    "            \n",
    "            if instance['CurrentTurn'] == prev_turn:\n",
    "                continue\n",
    "            else:\n",
    "                n_words_in_target_turn.append(len(text.split(' ')))\n",
    "                prev_turn = instance['CurrentTurn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88944670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(n_words_in_target_turn), np.std(n_words_in_target_turn), np.median(n_words_in_target_turn), \n",
    "      np.min(n_words_in_target_turn), np.max(n_words_in_target_turn))\n",
    "\n",
    "fig = sns.histplot(n_words_in_target_turn, log_scale=True, bins=6)\n",
    "fig.set(xlabel='Log number of words in a turn containing a construction')\n",
    "plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bccd5879",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "_lens = [round(np.mean(n_words_in_target_turn) + x * np.std(n_words_in_target_turn)) for x in range(11)]\n",
    "\n",
    "for i, _len in enumerate(_lens):\n",
    "    _percent = 100 * len([x for x in n_words_in_target_turn if x<=_len]) / len(n_words_in_target_turn)\n",
    "    print('{:.2f}% of turns have at most {:3d} words (mean + {:2d} * std) '.format(_percent, _len, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ed79e",
   "metadata": {},
   "source": [
    "## Number of constructions in a dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_dial = list(ds.n_tokens_in_dial.values())\n",
    "np.min(n_in_dial), np.max(n_in_dial), np.mean(n_in_dial), np.std(n_in_dial), np.median(n_in_dial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "constr_freqs = []\n",
    "for dial in data:\n",
    "    for constr in data[dial]:\n",
    "        freq_ = data[dial][constr][0]['Frequency']\n",
    "        assert freq_ == len(data[dial][constr])\n",
    "        constr_freqs.append(freq_)\n",
    "\n",
    "np.min(constr_freqs), np.max(constr_freqs), np.mean(constr_freqs), np.std(constr_freqs), np.median(constr_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b79f8",
   "metadata": {},
   "source": [
    "## Total number of constructions (tokens and types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = set()\n",
    "n_tokens = 0\n",
    "for dial in data:\n",
    "    for constr in data[dial]:\n",
    "        types.add(constr)\n",
    "        n_tokens += len(data[dial][constr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3999fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens, len(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcf394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
