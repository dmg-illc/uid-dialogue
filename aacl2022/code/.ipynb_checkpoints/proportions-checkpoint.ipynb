{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b699dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import json \n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data_dir = '/Users/mario/code/exp-rep/data/BNC-2014/two-speakers/analysis/'\n",
    "dialign_output = analysis_data_dir + 'dialign-output/'\n",
    "\n",
    "# dialign_output += 'nopos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73521285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 187\n"
     ]
    }
   ],
   "source": [
    "shared_lexica = {}\n",
    "for f in os.listdir(dialign_output + 'nopos/'):\n",
    "    if f.endswith('_tsv-lexicon.tsv') and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'nopos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in shared_lexica:\n",
    "            shared_lexica[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            shared_lexica[dial_id] = pd.concat([shared_lexica[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "self_lexica = {}\n",
    "for f in os.listdir(dialign_output + 'nopos/'):\n",
    "    if (f.endswith('_tsv-lexicon-self-rep-A.tsv') or f.endswith('_tsv-lexicon-self-rep-B.tsv')) and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'nopos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in self_lexica:\n",
    "            self_lexica[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            self_lexica[dial_id] = pd.concat([self_lexica[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(shared_lexica), len(self_lexica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca41764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 187\n"
     ]
    }
   ],
   "source": [
    "shared_lexica_pos = {}\n",
    "for f in os.listdir(dialign_output + 'pos/'):\n",
    "    if f.endswith('_tsv-lexicon.tsv') and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'pos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in shared_lexica_pos:\n",
    "            shared_lexica_pos[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            shared_lexica_pos[dial_id] = pd.concat([shared_lexica_pos[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "\n",
    "\n",
    "self_lexica_pos = {}\n",
    "for f in os.listdir(dialign_output + 'pos/'):\n",
    "    if (f.endswith('_tsv-lexicon-self-rep-A.tsv') or f.endswith('_tsv-lexicon-self-rep-B.tsv')) and not f.startswith('.'):\n",
    "        filepath = os.path.join(dialign_output + 'pos/', f)\n",
    "        dial_id = f.split('_')[0]\n",
    "        if dial_id not in self_lexica_pos:\n",
    "            self_lexica_pos[dial_id] = pd.read_csv(filepath, sep='\\t', header=0)\n",
    "        else:\n",
    "            self_lexica_pos[dial_id] = pd.concat([self_lexica_pos[dial_id], pd.read_csv(filepath, sep='\\t', header=0)])\n",
    "\n",
    "print(len(shared_lexica_pos), len(self_lexica_pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f169eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(analysis_data_dir + 'contexts.json', 'r') as f:\n",
    "    contexts = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a1eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq.</th>\n",
       "      <th>Free Freq.</th>\n",
       "      <th>Size</th>\n",
       "      <th>Surface Form</th>\n",
       "      <th>Establishment turn</th>\n",
       "      <th>Spanning</th>\n",
       "      <th>Priming</th>\n",
       "      <th>First Speaker</th>\n",
       "      <th>Turns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>you#PRON know#VERB what#PRON i#PRON mean#VERB ...</td>\n",
       "      <td>1096</td>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>S0530</td>\n",
       "      <td>205, 356, 599, 1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>i#PRON du#VERB n#ADV no#VERB if#CONJ you#PRON ...</td>\n",
       "      <td>643</td>\n",
       "      <td>439</td>\n",
       "      <td>1</td>\n",
       "      <td>S0530</td>\n",
       "      <td>205, 643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>i#PRON was#VERB like#ADV oh#INTERJ my#PRON god...</td>\n",
       "      <td>815</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>S0530</td>\n",
       "      <td>798, 815, 822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Freq.  Free Freq.  Size                                       Surface Form  \\\n",
       "0      4           4     7  you#PRON know#VERB what#PRON i#PRON mean#VERB ...   \n",
       "1      2           2     7  i#PRON du#VERB n#ADV no#VERB if#CONJ you#PRON ...   \n",
       "2      3           3     6  i#PRON was#VERB like#ADV oh#INTERJ my#PRON god...   \n",
       "\n",
       "   Establishment turn  Spanning  Priming First Speaker                Turns  \n",
       "0                1096       892        3         S0530  205, 356, 599, 1096  \n",
       "1                 643       439        1         S0530             205, 643  \n",
       "2                 815        25        1         S0530        798, 815, 822  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_lexica_pos['SJV7'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2853c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topical_or_referential(word_seq, pos_seq):\n",
    "    assert len(word_seq) == len(pos_seq), (word_seq, pos_seq)\n",
    "    \n",
    "    GENERIC_NOUNS = 'bit bunch fact god middle ones part rest side sort sorts stuff thanks loads lot lots kind kinds time times way ways problem problems thing things idea ideas reason reasons day days week weeks year years'\n",
    "    GENERIC_NOUNS = GENERIC_NOUNS.split(' ')\n",
    "    \n",
    "    if pos_seq.count('SUBST') >= 1:\n",
    "        is_generic = True\n",
    "        for w, tag in zip(word_seq, pos_seq):\n",
    "            if tag == 'SUBST' and w not in GENERIC_NOUNS:\n",
    "                is_generic = False\n",
    "                break\n",
    "        if not is_generic:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def more_than_half_filled_pauses(construction):\n",
    "    construction = construction.split(' ')\n",
    "    FILLED_PAUSES = ['huh', 'uh', 'erm', 'hm', 'mm', 'er']\n",
    "    n_filled_pauses = 0.\n",
    "    for w in construction:\n",
    "        if w in FILLED_PAUSES:\n",
    "            n_filled_pauses += 1\n",
    "    return n_filled_pauses >= len(construction) / 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaacabe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_than_half_filled_pauses('mm mm mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b201658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illegal construction: made . com#SUBST\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_constructions = {}\n",
    "pos_tagged_constructions_topical = {}\n",
    "\n",
    "for d_id in shared_lexica_pos:\n",
    "    lexicon_df = pd.concat((shared_lexica_pos[d_id], self_lexica_pos[d_id]))\n",
    "    for _, row in lexicon_df.iterrows():\n",
    "        constr = row['Surface Form']\n",
    "        \n",
    "        if not isinstance(constr, str):\n",
    "            continue\n",
    "        constr = constr.replace('? #STOP', '?#STOP')\n",
    "        constr = constr.strip()\n",
    "            \n",
    "        tokens = constr.split(' ')\n",
    "        w_seq = []\n",
    "        pos_seq = []\n",
    "        illegal_constr = False\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                w, tag = token.split('#')\n",
    "            except ValueError:\n",
    "                illegal_constr = True \n",
    "            w_seq.append(w)\n",
    "            pos_seq.append(tag)\n",
    "        \n",
    "        if illegal_constr:\n",
    "            print('Illegal construction:', constr)\n",
    "            continue   # only exception is: \"made . com#SUBST\"\n",
    "        \n",
    "        concat_tokens = ''.join(w_seq)\n",
    "        \n",
    "        # Referential or topical constructions?\n",
    "        if topical_or_referential(w_seq, pos_seq):\n",
    "            pos_tagged_constructions_topical[concat_tokens] = pos_seq\n",
    "        else:\n",
    "            pos_tagged_constructions[concat_tokens] = pos_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e87062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'S0530',\n",
       " \"whereas if you're kind of like i dunno if you're just like helping yourself like if you're like at a healthy weight and you're like stuff it's just like do you know what i mean ?\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts['SJV7']['205']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dacc0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subsequence(subsequence, sequence):\n",
    "    try:\n",
    "        l = len(subsequence)\n",
    "    except TypeError:\n",
    "        print(subsequence)\n",
    "    ranges = []\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i+l] == subsequence:\n",
    "            if i - 1 < 0:\n",
    "                space_before = True\n",
    "            else:\n",
    "                space_before = sequence[i-1] in \" ',.!:;?\"\n",
    "  \n",
    "            if i + l >= len(sequence):\n",
    "                space_after = True\n",
    "            else:\n",
    "                space_after = sequence[i+l] in \" ',.!:;?\"\n",
    "                \n",
    "            if space_before and space_after:\n",
    "                ranges.append((i, i+l))\n",
    "    return ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2003570d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 19)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = \"and i was just like\"\n",
    "s = \"and i was just like oh my god\"\n",
    "find_subsequence(ss, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c27653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 22)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = \"bad for you\"\n",
    "s = \"yeah it is bad for you bad for your teeth\"\n",
    "find_subsequence(ss, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409720e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/847v13g104z23kr193w_v2640000gn/T/ipykernel_40390/56403121.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for d_id in tqdm(shared_lexica):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfe0aeb59d24755becce95bee74db16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_counts = defaultdict(lambda: {}) #[dialogue][expression]\n",
    "_data = defaultdict(lambda: {})\n",
    "\n",
    "cnt = []\n",
    "for d_id in tqdm(shared_lexica):\n",
    "    lexicon_df = pd.concat((shared_lexica[d_id], self_lexica[d_id]))\n",
    "    dialogue = contexts[d_id]\n",
    "    \n",
    "    for _, row in lexicon_df.iterrows():\n",
    "        constr = row['Surface Form']\n",
    "        \n",
    "        if not isinstance(constr, str):\n",
    "            continue\n",
    "        constr = constr.strip()\n",
    "            \n",
    "        turns = row['Turns'].split(', ')\n",
    "        \n",
    "        _freq = 0\n",
    "        for turn in turns:\n",
    "            _, _, text = dialogue[turn]\n",
    "            ranges = find_subsequence(constr, text)\n",
    "            _freq += len(ranges)\n",
    "        \n",
    "        assert _freq >= row['Freq.']\n",
    "        \n",
    "        # Condition 1: at least 3 tokens long\n",
    "        if row['Size'] < 3:\n",
    "            continue\n",
    "        \n",
    "        # Condition 2: frequency >= 3 in the dialogue\n",
    "        if _freq < 3:\n",
    "            continue\n",
    "            \n",
    "        # Condition 3: free form frequency >= 2 in the dialogue\n",
    "        if row['Free Freq.'] < 2:\n",
    "            continue\n",
    "        \n",
    "        concat_tokens = constr.replace(' ', '')\n",
    "        if concat_tokens in pos_tagged_constructions_topical:\n",
    "            _pos_seq = pos_tagged_constructions_topical[concat_tokens]\n",
    "            topical = True\n",
    "            cnt.append(1)\n",
    "        elif concat_tokens in pos_tagged_constructions:\n",
    "            _pos_seq = pos_tagged_constructions[concat_tokens]\n",
    "            topical = False\n",
    "            cnt.append(1)\n",
    "        else:\n",
    "            # Skip constructions for which we find no POS-tagged equivalent\n",
    "            cnt.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Condition 4: no punctuation in the construction\n",
    "        if \"STOP\" in _pos_seq:\n",
    "            continue\n",
    "            \n",
    "        # Condition 5: at least half of the construction should not correspond to filled pauses\n",
    "        if more_than_half_filled_pauses(constr):\n",
    "            continue\n",
    "\n",
    "        _data[d_id][constr] = {\n",
    "            'Frequency': _freq,\n",
    "            'Free frequency': row['Free Freq.'],\n",
    "            'Length': row['Size'],\n",
    "            'POS sequence': _pos_seq,\n",
    "            'First speaker': row['First Speaker'],\n",
    "            'Turns': turns, \n",
    "            'Spanning turns': row['Spanning'],\n",
    "            'Establishment turn': row['Establishment turn'],\n",
    "            'Topical': topical\n",
    "        }\n",
    "        corpus_counts[d_id][constr] = _freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0993b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 139 out of 19469 constructions (0.71%) as we find no POS-tagged equivalent.\n"
     ]
    }
   ],
   "source": [
    "print('Skipped {} out of {} constructions ({:.2f}%) as we find no POS-tagged equivalent.'.format(\n",
    "    len([x for x in cnt if x == 0]),\n",
    "    len(cnt),\n",
    "    len([x for x in cnt if x == 0]) / len(cnt) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12b5b245",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_data = defaultdict(lambda: defaultdict(lambda: {'Topical': 0, 'Non-topical': 0}))\n",
    "\n",
    "for dial_id in _data:\n",
    "\n",
    "    for c in _data[dial_id]:\n",
    "        topical_str = 'Topical' if _data[dial_id][c]['Topical'] else 'Non-topical'\n",
    "        c_len = _data[dial_id][c]['Length']\n",
    "        for turn_id in _data[dial_id][c]['Turns']:\n",
    "            c_data[dial_id][turn_id][topical_str] += c_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172da005",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "props = []\n",
    "\n",
    "for dial_id in contexts:\n",
    "    dialogue = contexts[dial_id]\n",
    "    for utt_idx in dialogue:\n",
    "        _, _, utt = dialogue[utt_idx]\n",
    "        total_tokens = len(utt.split())\n",
    "        non_constr = total_tokens - c_data[dial_id][utt_idx]['Topical'] - c_data[dial_id][utt_idx]['Non-topical']\n",
    "\n",
    "        props.append((\n",
    "            dial_id,\n",
    "            int(utt_idx),\n",
    "            int(utt_idx) / len(dialogue),\n",
    "            int(utt_idx) / len(dialogue) < 0.5,\n",
    "            c_data[dial_id][utt_idx]['Topical'],\n",
    "            c_data[dial_id][utt_idx]['Non-topical'],\n",
    "            c_data[dial_id][utt_idx]['Topical'] + c_data[dial_id][utt_idx]['Non-topical'],\n",
    "            non_constr,\n",
    "            c_data[dial_id][utt_idx]['Topical'] / total_tokens,\n",
    "            c_data[dial_id][utt_idx]['Non-topical'] / total_tokens,\n",
    "            (c_data[dial_id][utt_idx]['Topical'] + c_data[dial_id][utt_idx]['Non-topical']) / total_tokens,\n",
    "            non_constr / total_tokens,\n",
    "        ))\n",
    "\n",
    "prop_df = pd.DataFrame(props, columns=['dial_id', 'uttID', 'uttIDprop', 'bin', 'nTopical', 'nNonTopical', 'nConstr', 'nRest', 'propTopical', 'propNonTopical', 'propConstr', 'propRest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89aad4bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prop_df.to_csv('proportions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcf394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
