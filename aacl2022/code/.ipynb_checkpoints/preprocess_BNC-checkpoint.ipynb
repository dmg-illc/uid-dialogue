{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../src/')\n",
    "from BNC import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../erp/data/bnc2014spoken-xml/spoken/'\n",
    "out_path = '../../data/BNC-2014/two-speakers/analysis/dialign-format/'\n",
    "n_speakers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spoken BNC (Love et al., 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of XML files: 1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [03:55,  5.32it/s]                    \n"
     ]
    }
   ],
   "source": [
    "logger.info('Load corpus from {}'.format(data_path))\n",
    "corpus = Corpus(\n",
    "    untagged_path=os.path.join(data_path, \"untagged\"),\n",
    "    tagged_path=os.path.join(data_path, \"tagged\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split corpus for finetuning and analysis \n",
    "In the next cell, you can load the splits. The code to obtain the splits is provided below the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "187\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/BNC-2014/two-speakers/finetuning_ids.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    assert len(lines) == 1\n",
    "    finetuning_ids = lines[0].split(',')\n",
    "    print(len(finetuning_ids))\n",
    "    \n",
    "with open('../../data/BNC-2014/two-speakers/analysis_ids.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    assert len(lines) == 1\n",
    "    analysis_ids = lines[0].split(',')\n",
    "    print(len(analysis_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words_in_turn = []\n",
    "n_words_in_dial = []\n",
    "n_turns_in_dial = []\n",
    "\n",
    "for d_id in finetuning_ids + analysis_ids:\n",
    "    _n_words = 0\n",
    "    _n_turns = 0\n",
    "    for utt in corpus.conversations[d_id].utterances:\n",
    "        _len = len(utt.tokens)\n",
    "        n_words_in_turn.append(_len)\n",
    "        _n_words += _len\n",
    "        _n_turns += 1\n",
    "        \n",
    "    n_words_in_dial.append(_n_words)\n",
    "    n_turns_in_dial.append(_n_turns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736.1864951768489 599.347456472185 541.5 67 4859\n",
      "7752.729903536978 5596.089452337154 6102.0 819 39575\n",
      "10.530931977602489 15.0506085700599 6.0 0 982\n"
     ]
    }
   ],
   "source": [
    "for _list in [n_turns_in_dial, n_words_in_dial, n_words_in_turn]:\n",
    "    print(np.mean(_list), np.std(_list), np.median(_list), np.min(_list), np.max(_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contexts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-da82c41928ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_turns_in_dial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdial_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0m_n_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_n_turns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'contexts' is not defined"
     ]
    }
   ],
   "source": [
    "n_words_in_turn = []\n",
    "n_words_in_dial = []\n",
    "n_turns_in_dial = []\n",
    "\n",
    "for dial_id in contexts:\n",
    "    _n_words = 0\n",
    "    _n_turns = 0\n",
    "    \n",
    "    for _, turn in contexts[dial_id].items():\n",
    "        _len = len(turn[2].split(' '))\n",
    "        \n",
    "        n_words_in_turn.append(_len)\n",
    "        _n_words += _len\n",
    "        _n_turns += 1\n",
    "        \n",
    "    n_words_in_dial.append(_n_words)\n",
    "    n_turns_in_dial.append(_n_turns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_conv_ids = []\n",
    "\n",
    "for conv_id, conv in corpus.conversations.items():\n",
    "    if conv.n_speakers == n_speakers:\n",
    "        tmp_conv_ids.append(conv_id)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(tmp_conv_ids)\n",
    "\n",
    "finetuning_ids, analysis_ids = np.split(tmp_conv_ids, [int(len(tmp_conv_ids) * 0.7)])\n",
    "\n",
    "len(tmp_conv_ids), len(finetuning_ids), len(analysis_ids)\n",
    "\n",
    "with open('../../data/BNC-2014/two-speakers/finetuning_ids.csv', 'w') as f:\n",
    "    f.write(','.join(finetuning_ids))\n",
    "    \n",
    "with open('../../data/BNC-2014/two-speakers/analysis_ids.csv', 'w') as f:\n",
    "    f.write(','.join(analysis_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert analysis split to `dialign` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6605f0dc37460a8a462485d258d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for conv_id in tqdm(analysis_ids):\n",
    "    tsv_content = ''\n",
    "    for u in corpus.conversations[conv_id].utterances:\n",
    "        if not u.sentence:\n",
    "            continue\n",
    "        \n",
    "      # TODO! is it ok to just skip these utterances?\n",
    "        if u.speaker_id in ['UNKFEMALE', 'UNKMALE', 'UNKMULTI']:\n",
    "            continue\n",
    "            \n",
    "        new_line = '{}:\\t{}\\n'.format(u.speaker_id, u.sentence.strip().lower())\n",
    "        tsv_content += new_line\n",
    "    \n",
    "    filename = '{}.tsv'.format(conv_id)\n",
    "    with open(os.path.join(out_path, 'nopos', filename), 'w') as f_out:\n",
    "        f_out.write(tsv_content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29889903592f44aabe64b9d2cfc7ccc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for conv_id in tqdm(analysis_ids):\n",
    "    tsv_content = ''\n",
    "    for u in corpus.conversations[conv_id].utterances:\n",
    "        if not u.sentence:\n",
    "            continue\n",
    "        \n",
    "      # TODO! is it ok to just skip these utterances?\n",
    "        if u.speaker_id in ['UNKFEMALE', 'UNKMALE', 'UNKMULTI']:\n",
    "            continue\n",
    "        \n",
    "        sentence = ''\n",
    "        for t in u.tokens:\n",
    "            sentence += '{}#{} '.format(t.form.strip().lower(), t.word_class)\n",
    "            \n",
    "        new_line = '{}:\\t{}\\n'.format(u.speaker_id, sentence.strip())\n",
    "        tsv_content += new_line\n",
    "    \n",
    "    filename = '{}.tsv'.format(conv_id)\n",
    "    with open(os.path.join(out_path, 'pos', filename), 'w') as f_out:\n",
    "        f_out.write(tsv_content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store finetuning and analysis split to csv format for LM training and evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out_path_txt = '../../data/BNC-2014/two-speakers/finetuning/txt/'\n",
    "\n",
    "whole_dataset = ''\n",
    "for conv_id in tqdm(finetuning_ids):\n",
    "    dial_dataset = ''\n",
    "    \n",
    "    for u in corpus.conversations[conv_id].utterances:\n",
    "        if not u.sentence:\n",
    "            continue\n",
    "        \n",
    "#         if u.speaker_id in ['UNKFEMALE', 'UNKMALE', 'UNKMULTI']:\n",
    "#             continue\n",
    "        \n",
    "        new_line = '{}\\n'.format(u.sentence.strip())\n",
    "        whole_dataset += new_line\n",
    "        dial_dataset += new_line\n",
    "    \n",
    "    filename = '{}.txt'.format(conv_id)\n",
    "    with open(os.path.join(out_path_txt, filename), 'w') as f_out:\n",
    "        f_out.write(dial_dataset)\n",
    "        \n",
    "with open(os.path.join(out_path_txt, 'all.txt'), 'w') as f_out:\n",
    "    f_out.write(whole_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "out_path_txt = '../../data/BNC-2014/two-speakers/analysis/txt/'\n",
    "\n",
    "whole_dataset = ''\n",
    "for conv_id in tqdm(analysis_ids):\n",
    "    dial_dataset = ''\n",
    "    \n",
    "    for u in corpus.conversations[conv_id].utterances:\n",
    "        if not u.sentence:\n",
    "            continue\n",
    "        \n",
    "#         if u.speaker_id in ['UNKFEMALE', 'UNKMALE', 'UNKMULTI']:\n",
    "#             continue\n",
    "        \n",
    "        new_line = '{}\\n'.format(u.sentence.strip())\n",
    "        whole_dataset += new_line\n",
    "        dial_dataset += new_line\n",
    "    \n",
    "    \n",
    "    filename = '{}.txt'.format(conv_id)\n",
    "    with open(os.path.join(out_path_txt, filename), 'w') as f_out:\n",
    "        f_out.write(dial_dataset)\n",
    "        \n",
    "with open(os.path.join(out_path_txt, 'all.txt'), 'w') as f_out:\n",
    "    f_out.write(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare analysis data for learning rate selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "out_path_lr = '../../data/BNC-2014/two-speakers/analysis/lr/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SV2V',\n",
       " 'SFET',\n",
       " 'SF2F',\n",
       " 'SZBR',\n",
       " 'S6ZU',\n",
       " 'SG2E',\n",
       " 'SAXQ',\n",
       " 'S87R',\n",
       " 'SRRQ',\n",
       " 'SQ63',\n",
       " 'SLRY',\n",
       " 'SNRP',\n",
       " 'SJV7',\n",
       " 'SDEX',\n",
       " 'S38F',\n",
       " 'S9WZ',\n",
       " 'S9N4',\n",
       " 'SZBR',\n",
       " 'SRDJ',\n",
       " 'SDEX']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection_ids = np.random.choice(analysis_ids, 20).tolist()\n",
    "lr_selection_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dial in lr_selection_ids:\n",
    "    \n",
    "    dial_dataset = ''\n",
    "    \n",
    "    for u in corpus.conversations[dial].utterances:\n",
    "        if not u.sentence:\n",
    "            continue\n",
    "        new_line = '{}\\n'.format(u.sentence.strip())\n",
    "        dial_dataset += new_line\n",
    "    \n",
    "    filename = '{}.txt'.format(dial)\n",
    "    with open(os.path.join(out_path_lr, filename), 'w') as f_out:\n",
    "        f_out.write(dial_dataset)\n",
    "    \n",
    "    \n",
    "    remaining_ids = lr_selection_ids[:]\n",
    "    remaining_ids.remove(dial)\n",
    "    eval_dataset = ''\n",
    "    \n",
    "    for eval_dial in lr_selection_ids:\n",
    "        for u in corpus.conversations[dial].utterances:\n",
    "            if not u.sentence:\n",
    "                continue\n",
    "            new_line = '{}\\n'.format(u.sentence.strip())\n",
    "            eval_dataset += new_line\n",
    "    \n",
    "    filename = '{}-eval.txt'.format(dial)\n",
    "    with open(os.path.join(out_path_lr, filename), 'w') as f_out:\n",
    "        f_out.write(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Save finetuning and analysis dialogues as dataframes for info density estimates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences = []\n",
    "dialogue_ids = []\n",
    "positions = []\n",
    "n_speakers_list = []\n",
    "\n",
    "# Collect utterances and utterance position (turn)\n",
    "for conv_ids, split in [(val, 'analysis.csv'), (train, 'finetune.csv')]:\n",
    "    for conv_id in tqdm(conv_ids):\n",
    "        pos = 1\n",
    "        for u in corpus.conversations[conv_id].utterances:\n",
    "            if not u.sentence:\n",
    "                continue\n",
    "            sentences.append(u.sentence)\n",
    "            dialogue_ids.append(conv_id)\n",
    "            positions.append(pos)\n",
    "#             n_speakers_list.append(corpus.conversations[conv_id].n_speakers)\n",
    "            pos += 1\n",
    "\n",
    "    if sentences:\n",
    "        dataset = list(zip(positions, sentences, dialogue_ids))  # n_speakers_list, \n",
    "        dataframe = pd.DataFrame(\n",
    "            dataset,\n",
    "            columns=['position', 'text', 'dialogue_id']  # 'n_speakers'\n",
    "    )\n",
    "        dataframe.to_csv(os.path.join(out_path, split), index=None)\n",
    "        logger.info('Corpus saved to {}'.format(out_path))\n",
    "    else:\n",
    "        logger.warning('No sentences found with the provided script arguments.')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'{}/{}.csv'.format(out_path, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
